# üîß Month 9 ‚Äî Feature Engineering & Unsupervised Learning

**Duration:** 80 hours
**Difficulty:** ‚≠ê‚≠ê‚≠ê Hard
**Goal:** Automate feature creation and discover hidden patterns

## Sprints

### Sprint 1: Feature Scaling & Encoding (Days 1‚Äì5)
- StandardScaler, MinMaxScaler, RobustScaler
- One-hot encoding, label encoding
- Binary encoding
- Target encoding

### Sprint 2: Dimensionality Reduction (Days 6‚Äì12)
- Principal Component Analysis (PCA)
- t-SNE for visualization
- Feature selection (correlation, mutual information)
- Domain-specific feature selection

### Sprint 3: Unsupervised Learning - Clustering (Days 13‚Äì28)
- K-Means clustering
- DBSCAN
- Hierarchical clustering
- Choosing optimal k
- Silhouette analysis

### Sprint 4: Feature Pipelines (Days 29‚Äì35)
- Scikit-learn pipelines
- Column transformers
- Custom transformers
- Reproducible preprocessing

### Sprint 5: Advanced Techniques (Days 36‚Äì40)
- Polynomial features
- Interaction features
- Time series features (lags, rolling stats)
- Domain knowledge features

## Project: Customer Segmentation Dashboard

**Objective:** Segment customers and create actionable insights
**Skills:** Clustering, visualization, business interpretation
**Deliverables:**
- Customer segments (3-5 clusters)
- Segment profiles and characteristics
- Interactive dashboard or report
- Actionable recommendations

## Resources

- [Scikit-learn Feature Engineering](https://scikit-learn.org/stable/modules/feature_extraction.html)
- [Feature Engineering for ML](https://www.featuretools.com/)
- [Clustering Algorithms](https://scikit-learn.org/stable/modules/clustering.html)
- [PCA Explained](https://towardsdatascience.com)

## Checklist

- [ ] Master scaling and encoding
- [ ] Learn PCA and dimensionality reduction
- [ ] Understand clustering algorithms
- [ ] Build feature pipelines
- [ ] Complete segmentation project
- [ ] Create visualization dashboard
